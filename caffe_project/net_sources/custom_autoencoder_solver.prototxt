net: "custom_autoencoder.prototxt"
# Use adative gradient descent over static
solver_type: ADAGRAD
base_lr: 0.000001
# learning rate policy: drop the learning rate in "steps"
# by a factor of gamma every stepsize iterations
lr_policy: "step"
# drop the learning rate by a factor of 10
# (i.e., multiply it by a factor of gamma = 0.1)
gamma: 0.1
stepsize: 10000
# Show output every n iterations
display: 100
max_iter: 6000
# Multiply all future updates by this after 'stepsize'
# Multiply all future updates by weight_decay**2 after two 'stepsizes', etc..
weight_decay: 0.0005
# How many iterations inbetween saves
snapshot: 1000
# Directory to Save model to when training
snapshot_prefix: "model_states/custom_autoencoder"
# Coefficient of how much of the previous update we apply in next update
momentum: 0.9
# solver mode: CPU or GPU
solver_mode: CPU


# Documentation from solver.cpp
# Also see https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto for list of params

#  Return the current learning rate. The currently implemented learning rate
#  policies are as follows:
#     - fixed: always return base_lr.
#     - step: return base_lr * gamma ^ (floor(iter / step))
#     - exp: return base_lr * gamma ^ iter
#     - inv: return base_lr * (1 + gamma * iter) ^ (- power)
#     - multistep: similar to step but it allows non uniform steps defined by
#       stepvalue
#     - poly: the effective learning rate follows a polynomial decay, to be
#       zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)
#     - sigmoid: the effective learning rate follows a sigmod decay
#       return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))
